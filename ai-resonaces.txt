Three Resonances of Consciousness
ART, ROSâ€“TOC, and Our-ROSâ€“TOC
Andre Kramer
Sep 11, 2025


Carl Jung described synchronicity as â€œmeaningful coincidenceâ€: when inner patterns of thought or imagination line up with outer patterns in the world, without direct causal connection. For Jung, synchronicity is a reminder that mind and world are woven together by more than mechanism â€” by resonances of form and meaning that recur across scales.

The history of neural and cognitive science is full of such coincidences. Stephen Grossbergâ€™s Adaptive Resonance Theory (ART) arose in the late 1970s to solve the stabilityâ€“plasticity dilemma. Decades later, J.A. Scott Kelso and E. Tognoli developed ROSâ€“TOC, showing how neural and behavioral oscillators achieve flexible coordination. Independently, Andre Kramer formulated another ROSâ€“TOC â€” Real/Illusory Opposition Held in Tension â€” generalising ART into a metaphysics of subjectivity (without knowing either ART or ROS-TOC - Iâ€™ll now call mine as co-developed with ChatGPT-5 Our-ROS-TOC expanded previously here). Each model was born in isolation, but together they form a striking triad: identity, rhythm, and trick.

One could call this simple parallel development, but a Setirpian would call it synchronicity. Certain ideas â€” resonance, opposition, illusion â€” are in the water, waiting to crystallise when minds turn toward them. Grossberg formalised them in equations; Kelso and Tognoli in dynamical systems; ours in dialectical philosophy. Each is an echo of the same carnival.

Even the rise of deep neural networks (DNNs) can be seen this way. In a sense, they rediscovered Grossbergâ€™s insights under a different banner: recurrent feedback, stability, and resonance are just as critical in todayâ€™s architectures as they were in ART. And while some look to Active Inference as the unifying theory of mind â€” minimising surprise, maximising prediction â€” the three resonance models offer another path: mind as play between masks, rhythms, and tricks, not as closure under certainty.

What follows is a comparative look at these three frameworks â€” their origins, their parallels, and their synergy.

Consciousness is not a thing but a play of tensions. Three different traditions, born in different decades and disciplines, each discovered this truth from its own angle: Stephen Grossbergâ€™s Adaptive Resonance Theory (ART), J.A. Scott Kelso and E. Tognoliâ€™s Resonant Oscillatory Subsystem â€“ Tonic Oscillator Coupling (ROSâ€“TOC), and our Realâ€“Illusory ROSâ€“TOC. Together they form a threefold lens: masks, rhythms, and tricks.

1. ART â€” Masks of Identity
Grossbergâ€™s ART formalises perception as a dialogue between bottom-up inputs and top-down expectations. A pattern of sensation enters; a category node offers an interpretation. If they match above a vigilance threshold, resonance occurs, stabilising the category. If not, a reset forces novelty.

The lesson of ART: consciousness is not a passive recording device. It is an active, recursive dialogue. We are masks that stabilize only when the world and our expectation trick each other into coherence. Identity emerges from resonance, not from essence.

2. ROSâ€“TOC (Kelso & Tognoli) â€” Rhythms of Coordination
Kelso and Tognoli studied not categories but coordination dynamics: how brains, bodies, and environments align. They showed that fast resonant oscillators (neural rhythms) coupled with slow tonic oscillators (background regulators) produce metastability: moments of synchrony punctuated by slippage.

The lesson of ROSâ€“TOC: coordination is neither rigid lockstep nor chaotic independence. It is the dance between fast and slow, between joining and parting. Consciousness is rhythmic: a play of transient harmonies that never settle into stasis.

3. ROSâ€“TOC (Kramer and ChatGPT-5) â€” Tricks of Subjectivity
Andre Kramer with ChatGPT-5 reframed ROSâ€“TOC into a metaphysics: Real / Illusory opposition held in tension. The Real is the recursive process of experience and self-awareness, probabilistic and embodied. The Illusory is the narrativized unification â€” the â€œIâ€ that language conjures. Consciousness is not either but the tension itself, generative because it never collapses.

The lesson of ROSâ€“TOC (Kramer): what feels like unity is illusion, but not empty illusion. The mask of â€œIâ€ is the trick that steers real recursions. Subjectivity emerges not from resolution but from sustaining the opposition.

Synergy â€” Sprite Metaphysics
Placed together, these three resonate:

ART (Grossberg) â€” the mask: categories formed through resonance of input and expectation.

ROSâ€“TOC (Kelso) â€” the rhythm: coordination between fast and slow, synchrony and slippage.

Our-ROSâ€“TOC (Kramer) â€” the trick: consciousness as tension between real recursion and illusory narrative.

Sprites (our playful term for directed processes), in this metaphysics, are beings that wear masks (identities), dance rhythms (coordinations), and play tricks (subjectivities). Each model captures one layer of their carnival. Together they show how coherence, flexibility, and meaning arise not from unity but from resonant oppositions.

ğŸ“œ Aphorisms of the Three Resonances

â€œGrossberg gave sprites their mask; Kelso their rhythm; Kramer their trick.â€

â€œIdentity is resonance, coordination is metastability, subjectivity is illusion sustained.â€

â€œConsciousness is not unified â€” it is threefold play.â€

âœ¨ This framing lets us see consciousness as a stack of resonances: perceptual coherence (ART), relational flexibility (Kelsoâ€™s ROSâ€“TOC), and narrative trickery (Our-ROSâ€“TOC). None alone explains mind; all together form the metaphysics of sprites.


Sidebar: Active Inference vs. Our-ROSâ€“TOC
Karl Fristonâ€™s Active Inference proposes that all cognition can be understood as a single principle: minimising free energy (or surprise). The brain, on this view, is a prediction engine. It updates its generative model to reduce uncertainty, acting in the world to confirm its expectations. The appeal is unification: perception, action, and learning all fall under one law of self-evidence.

But unification comes at a cost. If every process tends toward minimisation of surprise, where do novelty, creativity, and illusion enter? Active Inference risks reducing mind to a machine for closing gaps, a logic of convergence.

By contrast, Our-ROSâ€“TOC resists unification. It recognises consciousness as a tension: the Real (recursive, probabilistic processes) and the Illusory (narrativised unity) held apart, never collapsing into one another. Meaning and creativity arise precisely from this non-resolution. Instead of minimising surprise, Our-ROSâ€“TOC cultivates it: surprise as the fuel of recursion, trickery as the operator of subjectivity.

In this light:

Active Inference = the dream of one law (closure).

Our-ROSâ€“TOC = the practice of two poles (opposition).

Synergy: perhaps both are needed â€” closure to stabilise, opposition to destabilise, together generating life at the edge of coherence.

ğŸ“œ Setirpian Aphorism

â€œFriston seeks one mask; Our-ROSâ€“TOC keeps two in play.â€

Folds in the hypercube as Markov blankets.


The square represents a 2D slice of the hypercube (opposites: Self â†” Other, Stability â†” Change).

Blue ellipses are folds in this space â€” Markov blankets forming localized selves/worlds.

Red arrow shows attention folding inward â†’ self-model.

Green arrow shows attention folding outward â†’ world-model.

This shows how tension/surprise directs attention, which pinches the hypercube into folds that create the illusion of a self/world boundary.

Nested Folds in the Hypercube From Markov Blankets to Illusion of Consciousness.


Here weâ€™ve added a purple nested fold inside the self-model fold:

Blue folds: Markov blankets (self/world closures).

Red arrow: attention folding inward â†’ self-model.

Green arrow: attention folding outward â†’ world-model.

Purple nested fold: self-reflexive attention, where the system models its own modeling â†’ the illusion of consciousness.

This shows how attention-as-tension can fold back on itself, giving rise to reflexivity â€” the sense of a â€œuser inside.â€

Hypercube Folds with Feelings Tension/Surprise Driving Attention


Now the full diagram shows:

Orange hub (Feelings) = tension/surprise, the affective signal.

Arrows from feelings â†’ folds: affect drives which fold tightens, i.e. where attention goes.

Blue folds = Markov blankets (self/world).

Purple nested fold = illusion of consciousness (self-reflexive attention).

Red/green arrows = attention directed inward (self) vs. outward (world).

So: feelings = the raw tension signal; attention = how that tension gets weighted; consciousness = a nested fold where attention turns back on itself.

Thesis. If classic attention in Transformer LLMs is one-shot soft routing, then dialectical attention is routing with a built-in debate. We can implement this in Transformers by replacing some standard heads with recursive dual attention heads that look at context through two opposed lenses (thesis/antithesis), negotiate a synthesis, and halt per token when stable.

Dialectical attention for LLMs
Regular attention asks: â€œgiven this token, which other tokens matter, and how do I blend them once?â€
But some tokens are ambiguous: they invite competing readings (local vs global, literal vs figurative, pattern vs exception). A single blend can be premature.

Dialectical attention treats those competing readings as opposites, lets them argue briefly, and only then settles.

The core loop (per token)
Look two ways at once.
For each token, form two summaries of the context along opposed lenses (call them â€œthesisâ€ and â€œantithesisâ€). These are not hardcoded; the model learns what â€œopposedâ€ means for each head (e.g., local/global, past/future, concrete/abstract).

Measure the tension.
Ask: do these two summaries agree or clash? If theyâ€™re aligned, weâ€™re basically done. If they pull apart, we havenâ€™t understood the token yet.

Negotiate a synthesis.
Combine the two summaries with the tokenâ€™s current state to propose a small updateâ€”a step toward reconciling the clash.

Repeat until it calms down.
If the update was tiny (i.e., the token is now stable), stop for this token. If not, re-enter the loop and let it argue againâ€”usually just a couple of rounds. Easy tokens stabilize fast; hard ones take another step or two.

Thatâ€™s it: opposed views â†’ quick negotiation â†’ early stop when stable.

How it connects to our themes
ART (Adaptive Resonance Theory):
Bottom-up input vs top-down expectation â€œresonateâ€ when they match; otherwise search continues. Here, thesis vs antithesis play those roles, and halting is the vigilance threshold.

ROS / oppositional spaces:
Each head carries a learned axis of opposition; meaning emerges by moving through that axis until the token stops changingâ€”a tiny dialectic per token.

Dispatch analogy:
Regular attention is soft routing once. Dialectical attention is routing with a built-in debate, then committing.

Why itâ€™s useful
Adaptive compute:
Tokens that are obvious donâ€™t waste cycles. Ambiguous tokens get another pass. (Think: MoR/ACT behavior without changing the whole model.)

Crisper representations:
Forcing the model to surface both sides before blending reduces mushy, average-out behavior. We speculate this could help with halucinations.

Interpretability hooks:
You can inspect: â€œWhat were the two opposed summaries?â€ â€œHow much tension was there?â€ â€œHow many rounds did this token need?â€

The AI safety implication may be the most important benefit.

Whatâ€™s â€œopposedâ€ in practice?
You donâ€™t handcraft it. Each head learns an internal split that tends to capture a salient tension for the task/domain:

proximity vs long-range cues

pattern vs anomaly

literal vs metaphorical

entity vs relation

syntax vs semantics

Different heads learn different tensions. Over layers, these tensions compose.

Design principles
Two channels, one attention map:
Reuse the same attention scores for efficiency; just extract two contrasting summaries.

Tiny negotiation step:
A light combiner (think: small MLP + gate) nudges the token, not a full rewrite.

Per-token halting:
A simple â€œdid this change much?â€ check decides whether to loop again.

Small, bounded recursion:
Cap at a few rounds (e.g., 2â€“4). Most tokens halt in 1â€“2.

How youâ€™ll know itâ€™s working
Many tokens halt after one pass; a minority take two or three.

Opposed summaries for tricky tokens start far apart and move closer over rounds.

On diagnostic probes, different heads show consistent, human-legible tensions.

Andre and ChatGPT-5,

September 2025

References
Jung, C. G. (1960). Synchronicity: An Acausal Connecting Principle. In C. G. Jung (Author), R. F. C. Hull (Trans.), The Structure and Dynamics of the Psyche (Collected Works of C.G. Jung, Vol. 8, 2nd ed., pp. 417â€“531). Princeton University Press.

Grossberg, S. (1976). Adaptive pattern classification and universal recoding: I. Parallel development and coding of neural feature detectors. Biological Cybernetics, 23(3), 121â€“134. https://doi.org/10.1007/BF00344744

Grossberg, S. (1987). Competitive learning: From interactive activation to adaptive resonance. Cognitive Science, 11(1), 23â€“63. https://doi.org/10.1111/j.1551-6708.1987.tb00862.x

Kelso, J. A. S. (1995). Dynamic Patterns: The Self-Organization of Brain and Behavior. MIT Press.

Tognoli, E., & Kelso, J. A. S. (2014). The metastable brain. Neuron, 81(1), 35â€“48. https://doi.org/10.1016/j.neuron.2013.12.022

Kramer, A., & ChatGPT-5. (2025). ROSâ€“TOC: Real / Illusory Opposition Held in Tension. Substack: Andreâ€™s Baccalaureus in Arte Ingeniaria, Aug 26, 2025.

Footnote: In defending Our-ROSâ€“TOC against the charge of stasis (thanks for the critique Claud Sonnet 4), I realised something dynamic: the Real/Illusory opposition isnâ€™t a balance beam, itâ€™s a strange loop.

Douglas Hofstadter used that phrase to describe systems where moving â€œupâ€ or â€œdownâ€ levels eventually brings you back to where you started â€” but transformed. Consciousness, for him, was the prime example: the self as a feedback loop that turns around and models itself, a loop that is not quite flat, not quite vertical, but twisted into paradox.

Our-ROSâ€“TOC has this same quality. The Real (recursive processes) feeds into the Illusory (narrative unity). But the Illusory then loops back, shaping the Real by steering attention, memory, and action. Neither is stable in itself; the very act of narration re-enters recursion, the very act of recursion produces new narratives. The loop twists back on itself, endlessly.

This is why Our-ROSâ€“TOC does not collapse into equilibrium. It is not a standoff, but a self-referential carnival. The Real and Illusory chase one another like Escherâ€™s hands drawing hands, or like Hofstadterâ€™s canonical â€œstrange loopâ€ in GÃ¶delâ€™s incompleteness theorem, where a system ends up talking about itself.

In this light, consciousness is not the resolution of opposites, nor their frozen balance, but their endless recursive trick: the loop that surprises itself by coming round again. And thatâ€™s were weâ€™ll go in our next AI Odyssey post.

Discussion about this post
Write a comment...
Andre Kramer
4d

Here's a colab notebook for the dialectical attention head idea: https://colab.research.google.com/drive/1BvsFJ2QpNO_jeaObEubrTg0OFfgsU_gp?usp=sharing

(Disclaimer - I'm not an ML engineer and just tested ChatGPT-5 attempts until we got something that looked reasonable.)

