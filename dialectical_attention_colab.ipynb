{"nbformat": 4, "nbformat_minor": 5, "metadata": {"colab": {"name": "Dialectical_Attention_Demo.ipynb"}, "kernelspec": {"name": "python3", "display_name": "Python 3"}}, "cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Dialectical (Recursive Dual) Attention \u2014 Colab Demo\n", "\n", "This notebook implements a minimal **recursive dual attention head** (\"dialectical attention\") and plugs it into a tiny Transformer.\n", "\n", "It trains on a small **balanced-parentheses next-token prediction** task, then visualizes:\n", "- per-token **tension** (how opposed the two summaries are),\n", "- **steps used** by the per-token halting loop.\n", "\n", "You can use a GPU runtime in Colab (Runtime \u2192 Change runtime type \u2192 GPU) for faster training.\n"]}, {"cell_type": "code", "metadata": {"id": "setup"}, "execution_count": null, "outputs": [], "source": ["import math, random, os, sys\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import matplotlib.pyplot as plt\n", "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n", "device"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Dialectical Attention Head (single head)\n", "\n", "Key ideas:\n", "- One attention map, **two opposed value channels** (pos/neg).\n", "- Compute two summaries, measure **tension** (how opposed),\n", "- Propose a small **synthesis** update and **gate** it by tension,\n", "- **Recurse per token** until change is small or a max step is hit."]}, {"cell_type": "code", "metadata": {"id": "head"}, "execution_count": null, "outputs": [], "source": ["class DialecticalHead(nn.Module):\n", "    def __init__(self, d_model: int, d_head: int, max_steps: int = 3, halt_eps: float = 1e-3):\n", "        super().__init__()\n", "        self.q = nn.Linear(d_model, d_head, bias=False)\n", "        self.k = nn.Linear(d_model, d_head, bias=False)\n", "        self.v = nn.Linear(d_model, d_head, bias=False)\n", "        # Opposed value channels (+ / -)\n", "        self.v_pos = nn.Linear(d_head, d_head, bias=False)\n", "        self.v_neg = nn.Linear(d_head, d_head, bias=False)\n", "        # Tiny synthesis block\n", "        self.synth = nn.Linear(3 * d_head, d_head)\n", "        # Learned biases to tilt pos/neg routing\n", "        self.b_pos = nn.Parameter(torch.zeros(1))\n", "        self.b_neg = nn.Parameter(torch.zeros(1))\n", "        # Gate to scale updates\n", "        self.gate = nn.Linear(d_head, 1)\n", "        self.max_steps = max_steps\n", "        self.halt_eps = halt_eps\n", "\n", "    def forward(self, x, mask=None):\n", "        \"\"\"\n", "        x: [B, T, d_model]\n", "        mask: [B, 1, T, T] (0 for keep, -inf for block)\n", "        returns: [B, T, d_head], metrics\n", "        \"\"\"\n", "        B, T, _ = x.shape\n", "        q = self.q(x)\n", "        k = self.k(x)\n", "        v = self.v(x)\n", "        v_pos = self.v_pos(v)\n", "        v_neg = self.v_neg(v)\n", "\n", "        logits = torch.matmul(q, k.transpose(-1, -2)) / (k.size(-1) ** 0.5)\n", "        if mask is not None:\n", "            logits = logits + mask.squeeze(1)\n", "        attn_pos = F.softmax(logits + self.b_pos, dim=-1)\n", "        attn_neg = F.softmax(logits + self.b_neg, dim=-1)\n", "\n", "        z = q.clone()\n", "        active = torch.ones(B, T, dtype=torch.bool, device=x.device)\n", "        steps_used = torch.zeros(B, T, device=x.device)\n", "        mean_tensions = []\n", "\n", "        for t in range(self.max_steps):\n", "            up = torch.matmul(attn_pos, v_pos)\n", "            un = torch.matmul(attn_neg, v_neg)\n", "            # Tension proxy: negative cosine (higher when opposed)\n", "            cos = F.cosine_similarity(up, un, dim=-1, eps=1e-6).unsqueeze(-1)\n", "            tension = torch.sigmoid(-cos)\n", "            mean_tensions.append(tension.mean().item())\n", "\n", "            proposal = F.silu(self.synth(torch.cat([up, un, z], dim=-1)))\n", "            gate = torch.sigmoid(self.gate(z)) * tension\n", "            z_new = z + gate * proposal\n", "\n", "            delta = (z_new - z).norm(dim=-1) / (z.norm(dim=-1) + 1e-6)\n", "            newly_done = (delta < self.halt_eps)\n", "            active = active & (~newly_done)\n", "            z = z_new\n", "            steps_used += (~newly_done).float()\n", "\n", "            if not active.any():\n", "                break\n", "\n", "        metrics = {\n", "            'avg_steps': steps_used.mean().item(),\n", "            'tensions': mean_tensions,\n", "        }\n", "        return z, metrics\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Tiny Transformer Block using the Dialectical Head\n", "We keep it minimal: one dialectical head + residual + feedforward."]}, {"cell_type": "code", "metadata": {"id": "block"}, "execution_count": null, "outputs": [], "source": ["class TinyDialecticalBlock(nn.Module):\n", "    def __init__(self, d_model=128, d_head=64, max_steps=3, halt_eps=1e-3, ff_mult=2):\n", "        super().__init__()\n", "        self.attn = DialecticalHead(d_model, d_head, max_steps, halt_eps)\n", "        self.out = nn.Linear(d_head, d_model)\n", "        self.ln1 = nn.LayerNorm(d_model)\n", "        self.ff = nn.Sequential(\n", "            nn.LayerNorm(d_model),\n", "            nn.Linear(d_model, ff_mult*d_model),\n", "            nn.SiLU(),\n", "            nn.Linear(ff_mult*d_model, d_model),\n", "        )\n", "\n", "    def forward(self, x, mask=None):\n", "        z, metrics = self.attn(self.ln1(x), mask)\n", "        x = x + self.out(z)\n", "        x = x + self.ff(x)\n", "        return x, metrics\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Synthetic dataset: Balanced parentheses next-token prediction\n", "- Vocabulary: `\"()\"` plus a few filler symbols.\n", "- Generate balanced strings (Dyck(1)) of random lengths.\n", "- Task: next token prediction (language modeling)."]}, {"cell_type": "code", "metadata": {"id": "data"}, "execution_count": null, "outputs": [], "source": ["VOCAB = list(\"()abc \")  # include some distractors and space\n", "stoi = {ch:i for i,ch in enumerate(VOCAB)}\n", "itos = {i:ch for ch,i in stoi.items()}\n", "vocab_size = len(VOCAB)\n", "\n", "def gen_balanced(n_pairs=4, fillers=True):\n", "    # simple recursive generator\n", "    if n_pairs == 0: return \"\"\n", "    left = random.randint(0, n_pairs-1)\n", "    right = n_pairs-1-left\n", "    inner = gen_balanced(left, fillers)\n", "    outer = gen_balanced(right, fillers)\n", "    s = '(' + inner + ')' + outer\n", "    if fillers:\n", "        # sprinkle filler chars\n", "        out = []\n", "        for ch in s:\n", "            out.append(ch)\n", "            if random.random() < 0.2:\n", "                out.append(random.choice('ab '))\n", "        s = ''.join(out)\n", "    return s\n", "\n", "def make_sample(max_pairs=6, max_len=64):\n", "    pairs = random.randint(1, max_pairs)\n", "    s = gen_balanced(pairs)\n", "    s = s[:max_len-1]  # leave room for next-token target\n", "    x = torch.tensor([stoi[ch] for ch in s], dtype=torch.long)\n", "    y = torch.tensor([stoi[ch] for ch in (s[1:] + ' ')], dtype=torch.long)  # next-token target\n", "    return x, y\n", "\n", "def batchify(B=32, max_len=64):\n", "    xs, ys = [], []\n", "    for _ in range(B):\n", "        x, y = make_sample(max_len=max_len)\n", "        xs.append(x)\n", "        ys.append(y)\n", "    T = max(x.size(0) for x in xs)\n", "    X = torch.full((B, T), stoi[' '], dtype=torch.long)\n", "    Y = torch.full((B, T), stoi[' '], dtype=torch.long)\n", "    for i,(x,y) in enumerate(zip(xs, ys)):\n", "        X[i, :x.size(0)] = x\n", "        Y[i, :y.size(0)] = y\n", "    return X.to(device), Y.to(device)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Tiny model wrapper\n", "Embedding + positional encoding, one dialectical block, LM head."]}, {"cell_type": "code", "metadata": {"id": "model"}, "execution_count": null, "outputs": [], "source": ["class TinyDialecticalLM(nn.Module):\n", "    def __init__(self, vocab_size, d_model=128, d_head=64, max_steps=3, halt_eps=1e-3):\n", "        super().__init__()\n", "        self.emb = nn.Embedding(vocab_size, d_model)\n", "        self.pos = nn.Parameter(torch.randn(1, 256, d_model) * 0.01)  # max length 256\n", "        self.block = TinyDialecticalBlock(d_model, d_head, max_steps, halt_eps)\n", "        self.lm = nn.Linear(d_model, vocab_size)\n", "\n", "    def forward(self, x):\n", "        B, T = x.shape\n", "        h = self.emb(x) + self.pos[:, :T, :]\n", "        h, metrics = self.block(h)\n", "        logits = self.lm(h)\n", "        return logits, metrics\n", "\n", "model = TinyDialecticalLM(vocab_size).to(device)\n", "sum(p.numel() for p in model.parameters())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Train briefly\n", "Small run just to see loss go down and produce nontrivial tension/steps."]}, {"cell_type": "code", "metadata": {"id": "train"}, "execution_count": null, "outputs": [], "source": ["optim = torch.optim.AdamW(model.parameters(), lr=3e-3)\n", "losses = []\n", "for step in range(300):  # keep modest for Colab\n", "    X, Y = batchify(B=64, max_len=96)\n", "    logits, metrics = model(X)\n", "    loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), Y.reshape(-1))\n", "    optim.zero_grad(); loss.backward(); optim.step()\n", "    if step % 20 == 0:\n", "        print(f\"step {step:03d} loss {loss.item():.3f} avg_steps {metrics['avg_steps']:.2f}\")\n", "    losses.append(loss.item())\n", "plt.plot(losses); plt.title('Training loss'); plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Inspect tension & steps on a batch\n", "We gather per-token tension/steps and plot quick histograms."]}, {"cell_type": "code", "metadata": {"id": "inspect"}, "execution_count": null, "outputs": [], "source": ["X, Y = batchify(B=32, max_len=96)\n", "with torch.no_grad():\n", "    logits, metrics = model(X)\n", "print('Avg steps (approx):', metrics['avg_steps'])\n", "plt.plot(metrics['tensions']); plt.title('Mean tension per recursion step'); plt.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Qualitative sample\n", "Greedy next-token generation from a seed."]}, {"cell_type": "code", "metadata": {"id": "sample"}, "execution_count": null, "outputs": [], "source": ["def generate(seed='((', max_new=40):\n", "    x = torch.tensor([[stoi.get(ch, stoi[' ']) for ch in seed]], device=device)\n", "    for _ in range(max_new):\n", "        logits, _ = model(x)\n", "        nxt = logits[0, -1].argmax().unsqueeze(0).unsqueeze(0)\n", "        x = torch.cat([x, nxt], dim=1)\n", "    return ''.join(itos[int(i)] for i in x[0].tolist())\n", "\n", "print(generate('(()'))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["----\n", "**Notes:**\n", "- To compare, you can swap `TinyDialecticalBlock` with a vanilla attention block and observe differences in tension/steps (the vanilla one will have none).\n", "- You can also log how many tokens halt after 1, 2, or 3 steps by instrumenting `steps_used` more precisely.\n", "- For factual tasks, you can use the *tension/steps* signal to trigger retrieval/verification (not shown here)."]}]}