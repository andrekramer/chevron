{"nbformat": 4, "nbformat_minor": 5, "metadata": {"colab": {"name": "Dialectical_Attention_Minimal_v3n_plus.ipynb"}, "kernelspec": {"name": "python3", "display_name": "Python 3"}}, "cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Minimal Dialectical Attention Head (with constrained decoding)\n", "\n", "Opposed streams \u2192 dual attention \u2192 tension-gated recursive update with per-token halting.\n", "Includes a **constrained decoder** to avoid unbalanced parentheses and trailing PAD/space runs.\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["import math, random\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import matplotlib.pyplot as plt\n", "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n", "device"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["class DialecticalHead(nn.Module):\n", "    def __init__(self, d_model: int, d_head: int, max_steps: int = 6, halt_eps: float = 5e-4, tension_tau: float = 0.15, min_steps:int=1):\n", "        super().__init__()\n", "        self.q = nn.Linear(d_model, d_head, bias=False)\n", "        self.k = nn.Linear(d_model, d_head, bias=False)\n", "        self.v = nn.Linear(d_model, d_head, bias=False)\n", "        self.v_pos = nn.Linear(d_head, d_head, bias=False)\n", "        self.v_neg = nn.Linear(d_head, d_head, bias=False)\n", "        self.synth = nn.Linear(5 * d_head, d_head)\n", "        self.up = nn.Linear(d_head, d_model)\n", "        self.u_pos = nn.Parameter(torch.zeros(d_head, 1))\n", "        self.u_neg = nn.Parameter(torch.zeros(d_head, 1))\n", "        self.b_pos = nn.Parameter(torch.zeros(1))\n", "        self.b_neg = nn.Parameter(torch.zeros(1))\n", "        self.step_gate = nn.Linear(d_head, 1)\n", "        self.max_steps, self.min_steps = max_steps, min_steps\n", "        self.halt_eps, self.tension_tau = halt_eps, tension_tau\n", "        nn.init.kaiming_uniform_(self.v_pos.weight, a=math.sqrt(5))\n", "        with torch.no_grad():\n", "            self.v_neg.weight.copy_(-self.v_pos.weight + 0.01 * torch.randn_like(self.v_pos.weight))\n", "\n", "    @staticmethod\n", "    def _masks(token_mask: torch.Tensor, T: int, device):\n", "        keep = torch.tril(torch.ones(T, T, device=device))\n", "        causal_add = torch.where(keep > 0, torch.zeros_like(keep), torch.full_like(keep, float('-inf')))  # [T,T]\n", "        key_add = (~token_mask).float().unsqueeze(1) * float('-inf')  # [B,1,T]\n", "        return causal_add, key_add\n", "\n", "    @staticmethod\n", "    def _bmm(attn: torch.Tensor, vals: torch.Tensor) -> torch.Tensor:\n", "        return torch.bmm(attn, vals)\n", "\n", "    def forward(self, x, token_mask):\n", "        B, T, _ = x.shape\n", "        z = x\n", "        active = token_mask.clone()\n", "        steps_used = torch.zeros(B, T, device=x.device)\n", "        tensions = []\n", "        causal_add, key_add = self._masks(token_mask, T, x.device)\n", "\n", "        for t in range(self.max_steps):\n", "            steps_used = steps_used + active.float()\n", "            z_eff = torch.where(active.unsqueeze(-1), z, z.detach())\n", "            q = self.q(z_eff); k = self.k(z_eff); v = self.v(z_eff)\n", "            vpos = self.v_pos(v); vneg = self.v_neg(v)\n", "            logits = torch.matmul(q, k.transpose(-1, -2)) / (k.size(-1) ** 0.5)\n", "            logits = logits + causal_add + key_add\n", "            logits = torch.nan_to_num(logits, nan=0.0, posinf=1e9, neginf=-1e9)\n", "            all_blocked = torch.isneginf(logits).all(dim=-1, keepdim=True)\n", "            logits = torch.where(all_blocked, torch.zeros_like(logits), logits)\n", "            tilt_p = torch.matmul(q, self.u_pos); tilt_n = torch.matmul(q, self.u_neg)\n", "            attn_p = F.softmax(logits + self.b_pos + tilt_p, dim=-1)\n", "            attn_n = F.softmax(logits + self.b_neg + tilt_n, dim=-1)\n", "            attn_p = torch.nan_to_num(attn_p, nan=0.0); attn_n = torch.nan_to_num(attn_n, nan=0.0)\n", "            attn_p = attn_p / attn_p.sum(-1, keepdim=True).clamp_min(1e-9)\n", "            attn_n = attn_n / attn_n.sum(-1, keepdim=True).clamp_min(1e-9)\n", "            up = self._bmm(attn_p, vpos); un = self._bmm(attn_n, vneg)\n", "            cos = F.cosine_similarity(up, un, dim=-1, eps=1e-6).unsqueeze(-1)\n", "            tension = 0.5 * (1.0 - cos)\n", "            act = active.float().unsqueeze(-1)\n", "            tensions.append(((tension * act).sum() / act.sum().clamp(min=1.0)).detach().item())\n", "            step = torch.sigmoid(self.step_gate(q)) * tension.clamp(0,1)\n", "            proposal = self.up(F.silu(self.synth(torch.cat([up, un, up-un, up+un, q], dim=-1))))\n", "            z_new = z + step * proposal\n", "            delta = (z_new - z).norm(dim=-1) / (z.norm(dim=-1) + 1e-6)\n", "            allow_halt = (t + 1) >= self.min_steps\n", "            done = allow_halt & ((delta < self.halt_eps) | (tension.squeeze(-1) < self.tension_tau)) & token_mask\n", "            z = torch.where(done.unsqueeze(-1), z, z_new)\n", "            active = active & (~done)\n", "            if not active.any():\n", "                break\n", "\n", "        denom = token_mask.float().sum().clamp(min=1.0)\n", "        avg_steps = (steps_used * token_mask.float()).sum() / denom\n", "        return z, {'avg_steps': avg_steps.item(), 'tensions': tensions}\n", "\n", "class TinyDialecticalBlock(nn.Module):\n", "    def __init__(self, d_model=128, d_head=64, **kw):\n", "        super().__init__()\n", "        self.attn = DialecticalHead(d_model, d_head, **kw)\n", "        self.out = nn.Linear(d_model, d_model)\n", "        self.ln1 = nn.LayerNorm(d_model)\n", "        self.ff = nn.Sequential(\n", "            nn.LayerNorm(d_model),\n", "            nn.Linear(d_model, 2*d_model), nn.SiLU(), nn.Linear(2*d_model, d_model)\n", "        )\n", "    def forward(self, x, token_mask):\n", "        z, metrics = self.attn(self.ln1(x), token_mask)\n", "        x = x + self.out(z)\n", "        x = x + self.ff(x)\n", "        return x, metrics\n", "\n", "class TinyDialecticalLM(nn.Module):\n", "    def __init__(self, vocab_size, pad_idx, d_model=128, d_head=64, **kw):\n", "        super().__init__()\n", "        self.pad_idx = pad_idx\n", "        self.emb = nn.Embedding(vocab_size, d_model)\n", "        nn.init.normal_(self.emb.weight, mean=0.0, std=0.02)\n", "        self.pos = nn.Parameter(torch.randn(1, 256, d_model) * 0.01)\n", "        self.block = TinyDialecticalBlock(d_model, d_head, **kw)\n", "        self.lm = nn.Linear(d_model, vocab_size)\n", "    def forward(self, x):\n", "        B, T = x.shape\n", "        m = (x != self.pad_idx)\n", "        h = self.emb(x) + self.pos[:, :T, :]\n", "        h, metrics = self.block(h, m)\n", "        return self.lm(h), metrics\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["VOCAB = list(\"()abc _\")  # '_' is PAD\n", "stoi = {ch:i for i,ch in enumerate(VOCAB)}\n", "itos = {i:ch for ch,i in stoi.items()}\n", "PAD = stoi['_']\n", "vocab_size = len(VOCAB)\n", "\n", "def gen_balanced(n_pairs=4, fillers=True):\n", "    if n_pairs == 0: return \"\"\n", "    left = random.randint(0, n_pairs-1)\n", "    right = n_pairs-1-left\n", "    inner = gen_balanced(left, fillers)\n", "    outer = gen_balanced(right, fillers)\n", "    s = '(' + inner + ')' + outer\n", "    if fillers:\n", "        out = []\n", "        for ch in s:\n", "            out.append(ch)\n", "            if random.random() < 0.2:\n", "                out.append(random.choice('ab '))\n", "        s = ''.join(out)\n", "    return s\n", "\n", "def make_sample(max_pairs=6, max_len=96):\n", "    pairs = random.randint(1, max_pairs)\n", "    s = gen_balanced(pairs)\n", "    s = s[:max_len-1]\n", "    x = torch.tensor([stoi[ch] for ch in s], dtype=torch.long)\n", "    y = torch.tensor([stoi[ch] for ch in (s[1:] + ' ')], dtype=torch.long)\n", "    return x, y\n", "\n", "def batchify(B=64, max_len=96, device=device):\n", "    xs, ys = [], []\n", "    for _ in range(B):\n", "        x, y = make_sample(max_len=max_len)\n", "        xs.append(x); ys.append(y)\n", "    T = max(x.size(0) for x in xs)\n", "    X = torch.full((B, T), PAD, dtype=torch.long)\n", "    Y = torch.full((B, T), PAD, dtype=torch.long)\n", "    for i,(x,y) in enumerate(zip(xs, ys)):\n", "        X[i, :x.size(0)] = x\n", "        Y[i, :y.size(0)] = y\n", "    return X.to(device), Y.to(device)\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["model = TinyDialecticalLM(vocab_size, PAD, d_model=128, d_head=64, max_steps=6, tension_tau=0.15).to(device)\n", "optim = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n", "losses, avg_steps_hist, tstarts, tends = [], [], [], []\n", "for step in range(200):\n", "    X, Y = batchify(B=64, max_len=96)\n", "    logits, metrics = model(X)\n", "    ce = F.cross_entropy(logits.reshape(-1, logits.size(-1)), Y.reshape(-1), ignore_index=PAD)\n", "    optim.zero_grad(); ce.backward(); nn.utils.clip_grad_norm_(model.parameters(), 1.0); optim.step()\n", "    losses.append(ce.item()); avg_steps_hist.append(metrics['avg_steps'])\n", "    if metrics['tensions']:\n", "        tstarts.append(metrics['tensions'][0]); tends.append(metrics['tensions'][-1])\n", "    if step % 20 == 0:\n", "        ts = metrics['tensions']\n", "        print(f\"step {step:03d} | loss {ce.item():.3f} | avg_steps {metrics['avg_steps']:.2f} | tension start {ts[0] if ts else float('nan'):.3f} -> end {ts[-1] if ts else float('nan'):.3f}\")\n", "plt.plot(losses); plt.title('Training loss'); plt.show()\n", "plt.plot(avg_steps_hist); plt.title('Avg steps used (non-pad tokens)'); plt.show()\n", "if tstarts and tends:\n", "    plt.plot(tstarts, label='start'); plt.plot(tends, label='end'); plt.legend(); plt.title('Tension start vs end per batch'); plt.show()\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["X, Y = batchify(B=32, max_len=96)\n", "with torch.no_grad():\n", "    logits, metrics = model(X)\n", "print('Avg steps (approx):', metrics['avg_steps'])\n", "plt.plot(metrics['tensions']); plt.title('Mean tension over ACTIVE non-pad tokens per recursion step'); plt.show()"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["def generate_constrained(model, seed='((', max_new=40, stop_on_closed=True, close_tail=True, bias_close=5.0):\n", "    idx_l = stoi['(']; idx_r = stoi[')']; idx_sp = stoi[' ']; idx_pad = stoi['_']\n", "    x = torch.tensor([[stoi.get(ch, idx_pad) for ch in seed]], device=device)\n", "    depth = sum(1 if ch=='(' else -1 if ch==')' else 0 for ch in seed)\n", "    with torch.no_grad():\n", "        for t in range(max_new):\n", "            logits, _ = model(x)\n", "            logit = logits[0, -1].clone()\n", "            logit[idx_pad] = -1e-9\n", "            if depth <= 0:\n", "                logit[idx_r] = -1e-9\n", "            remaining = max_new - t\n", "            if depth > 0 and remaining <= depth:\n", "                logit[idx_r] = logit[idx_r] + bias_close\n", "            nxt = logit.argmax().unsqueeze(0).unsqueeze(0)\n", "            ch = itos[int(nxt)]\n", "            if ch == '(':\n", "                depth += 1\n", "            elif ch == ')':\n", "                depth -= 1\n", "            x = torch.cat([x, nxt], dim=1)\n", "            if stop_on_closed and depth == 0 and ch == ' ':\n", "                break\n", "    if close_tail and depth > 0:\n", "        closes = torch.tensor([[idx_r] * depth], device=device)\n", "        x = torch.cat([x, closes], dim=1)\n", "    return ''.join(itos[int(i)] for i in x[0].tolist())\n", "\n", "print(generate_constrained(model, '(()', max_new=40))"]}]}