{"nbformat": 4, "nbformat_minor": 5, "metadata": {"colab": {"name": "Dialectical_Attention_Demo_v2.ipynb"}, "kernelspec": {"name": "python3", "display_name": "Python 3"}}, "cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Dialectical (Recursive Dual) Attention \u2014 Colab Demo v2\n", "\n", "**What's new in v2:**\n", "- Two value streams are initialized as **opposites** and nudged by an opposition regularizer.\n", "- **Per-stream routing tilts** (learned) make pos/neg actually look at different neighborhoods.\n", "- **Tension metric** is now linear in cosine: `tension = 0.5*(1 - cos)` (0=aligned, 1=opposed).\n", "- **Per-token halting** truly freezes stable tokens between recursion steps.\n", "\n", "The rest matches v1: tiny Transformer on a balanced-parentheses toy LM task, quick plots of loss and mean tension per recursion step."]}, {"cell_type": "code", "metadata": {"id": "setup"}, "execution_count": null, "outputs": [], "source": ["import math, random, os\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import matplotlib.pyplot as plt\n", "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n", "device"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Dialectical Attention Head (single head, improved)"]}, {"cell_type": "code", "metadata": {"id": "head"}, "execution_count": null, "outputs": [], "source": ["class DialecticalHead(nn.Module):\n", "    def __init__(self, d_model: int, d_head: int, max_steps: int = 4, halt_eps: float = 2e-3, opp_lambda: float = 1e-3):\n", "        super().__init__()\n", "        self.q = nn.Linear(d_model, d_head, bias=False)\n", "        self.k = nn.Linear(d_model, d_head, bias=False)\n", "        self.v = nn.Linear(d_model, d_head, bias=False)\n", "        # Opposed value channels (+ / -)\n", "        self.v_pos = nn.Linear(d_head, d_head, bias=False)\n", "        self.v_neg = nn.Linear(d_head, d_head, bias=False)\n", "        # Tiny synthesis block\n", "        self.synth = nn.Linear(3 * d_head, d_head)\n", "        # Learned per-stream query tilts (broadcast over keys)\n", "        self.u_pos = nn.Parameter(torch.zeros(d_head, 1))\n", "        self.u_neg = nn.Parameter(torch.zeros(d_head, 1))\n", "        # Small learned biases\n", "        self.b_pos = nn.Parameter(torch.zeros(1))\n", "        self.b_neg = nn.Parameter(torch.zeros(1))\n", "        # Gate to scale updates\n", "        self.gate = nn.Linear(d_head, 1)\n", "        self.max_steps = max_steps\n", "        self.halt_eps = halt_eps\n", "        self.opp_lambda = opp_lambda\n", "\n", "        self._init_opposed_streams()\n", "\n", "    def _init_opposed_streams(self):\n", "        # Initialize v_pos randomly and set v_neg \u2248 -v_pos for genuine opposition at start\n", "        nn.init.kaiming_uniform_(self.v_pos.weight, a=math.sqrt(5))\n", "        with torch.no_grad():\n", "            self.v_neg.weight.copy_(-self.v_pos.weight + 0.01 * torch.randn_like(self.v_pos.weight))\n", "\n", "    def forward(self, x, mask=None, return_aux=False):\n", "        \"\"\"\n", "        x: [B, T, d_model]\n", "        mask: [B, 1, T, T] (0 keep, -inf block) optional\n", "        returns: [B, T, d_head], metrics, extra_loss (opposition regularizer)\n", "        \"\"\"\n", "        B, T, _ = x.shape\n", "        q = self.q(x)\n", "        k = self.k(x)\n", "        v = self.v(x)\n", "        v_pos = self.v_pos(v)\n", "        v_neg = self.v_neg(v)\n", "\n", "        logits = torch.matmul(q, k.transpose(-1, -2)) / (k.size(-1) ** 0.5)\n", "        if mask is not None:\n", "            logits = logits + mask.squeeze(1)\n", "\n", "        # Per-stream routing tilts (query-dependent, broadcast across keys)\n", "        tilt_pos = torch.matmul(q, self.u_pos)  # [B,T,1]\n", "        tilt_neg = torch.matmul(q, self.u_neg)\n", "\n", "        attn_pos = F.softmax(logits + self.b_pos + tilt_pos, dim=-1)\n", "        attn_neg = F.softmax(logits + self.b_neg + tilt_neg, dim=-1)\n", "\n", "        # Initial state and masks\n", "        z = q.clone()\n", "        active = torch.ones(B, T, dtype=torch.bool, device=x.device)\n", "        steps_used = torch.zeros(B, T, device=x.device)\n", "        mean_tensions = []\n", "\n", "        for t in range(self.max_steps):\n", "            up = torch.matmul(attn_pos, v_pos)     # [B,T,d]\n", "            un = torch.matmul(attn_neg, v_neg)\n", "            # Tension in [0,1]: 0 aligned, 1 opposed\n", "            cos = F.cosine_similarity(up, un, dim=-1, eps=1e-6).unsqueeze(-1)\n", "            tension = 0.5 * (1.0 - cos)\n", "            mean_tensions.append(tension.mean().item())\n", "\n", "            proposal = F.silu(self.synth(torch.cat([up, un, z], dim=-1)))\n", "            gate = torch.sigmoid(self.gate(z)) * tension\n", "            z_new = z + gate * proposal\n", "\n", "            # Per-token halting: freeze tokens whose update is small\n", "            delta = (z_new - z).norm(dim=-1) / (z.norm(dim=-1) + 1e-6)\n", "            newly_done = (delta < self.halt_eps)\n", "            # Update state; frozen tokens simply carry forward\n", "            z = torch.where(newly_done.unsqueeze(-1), z, z_new)\n", "            steps_used = steps_used + (~newly_done).float()\n", "            active = active & (~newly_done)\n", "\n", "            if not active.any():\n", "                break\n", "\n", "        metrics = {\n", "            'avg_steps': steps_used.mean().item(),\n", "            'tensions': mean_tensions,\n", "        }\n", "\n", "        # Opposition regularizer: encourage v_neg \u2248 -v_pos within head subspace\n", "        opp_reg = (self.v_pos.weight + self.v_neg.weight).pow(2).mean()\n", "        extra_loss = self.opp_lambda * opp_reg\n", "\n", "        if return_aux:\n", "            return z, metrics, extra_loss, (attn_pos, attn_neg)\n", "        return z, metrics, extra_loss\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Tiny Transformer Block and LM wrapper"]}, {"cell_type": "code", "metadata": {"id": "block_model"}, "execution_count": null, "outputs": [], "source": ["class TinyDialecticalBlock(nn.Module):\n", "    def __init__(self, d_model=128, d_head=64, max_steps=4, halt_eps=2e-3, opp_lambda=1e-3, ff_mult=2):\n", "        super().__init__()\n", "        self.attn = DialecticalHead(d_model, d_head, max_steps, halt_eps, opp_lambda)\n", "        self.out = nn.Linear(d_head, d_model)\n", "        self.ln1 = nn.LayerNorm(d_model)\n", "        self.ff = nn.Sequential(\n", "            nn.LayerNorm(d_model),\n", "            nn.Linear(d_model, ff_mult*d_model),\n", "            nn.SiLU(),\n", "            nn.Linear(ff_mult*d_model, d_model),\n", "        )\n", "\n", "    def forward(self, x, mask=None):\n", "        z, metrics, extra_loss = self.attn(self.ln1(x), mask)\n", "        x = x + self.out(z)\n", "        x = x + self.ff(x)\n", "        return x, metrics, extra_loss\n", "\n", "class TinyDialecticalLM(nn.Module):\n", "    def __init__(self, vocab_size, d_model=128, d_head=64, max_steps=4, halt_eps=2e-3, opp_lambda=1e-3):\n", "        super().__init__()\n", "        self.emb = nn.Embedding(vocab_size, d_model)\n", "        self.pos = nn.Parameter(torch.randn(1, 256, d_model) * 0.01)\n", "        self.block = TinyDialecticalBlock(d_model, d_head, max_steps, halt_eps, opp_lambda)\n", "        self.lm = nn.Linear(d_model, vocab_size)\n", "\n", "    def forward(self, x):\n", "        B, T = x.shape\n", "        h = self.emb(x) + self.pos[:, :T, :]\n", "        h, metrics, extra_loss = self.block(h)\n", "        logits = self.lm(h)\n", "        return logits, metrics, extra_loss\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Synthetic dataset: Balanced parentheses (Dyck-1) with distractors"]}, {"cell_type": "code", "metadata": {"id": "data"}, "execution_count": null, "outputs": [], "source": ["VOCAB = list(\"()abc \")\n", "stoi = {ch:i for i,ch in enumerate(VOCAB)}\n", "itos = {i:ch for ch,i in stoi.items()}\n", "vocab_size = len(VOCAB)\n", "\n", "def gen_balanced(n_pairs=4, fillers=True):\n", "    if n_pairs == 0: return \"\"\n", "    left = random.randint(0, n_pairs-1)\n", "    right = n_pairs-1-left\n", "    inner = gen_balanced(left, fillers)\n", "    outer = gen_balanced(right, fillers)\n", "    s = '(' + inner + ')' + outer\n", "    if fillers:\n", "        out = []\n", "        for ch in s:\n", "            out.append(ch)\n", "            if random.random() < 0.2:\n", "                out.append(random.choice('ab '))\n", "        s = ''.join(out)\n", "    return s\n", "\n", "def make_sample(max_pairs=6, max_len=96):\n", "    pairs = random.randint(1, max_pairs)\n", "    s = gen_balanced(pairs)\n", "    s = s[:max_len-1]\n", "    x = torch.tensor([stoi[ch] for ch in s], dtype=torch.long)\n", "    y = torch.tensor([stoi[ch] for ch in (s[1:] + ' ')], dtype=torch.long)\n", "    return x, y\n", "\n", "def batchify(B=64, max_len=96, device=device):\n", "    xs, ys = [], []\n", "    for _ in range(B):\n", "        x, y = make_sample(max_len=max_len)\n", "        xs.append(x); ys.append(y)\n", "    T = max(x.size(0) for x in xs)\n", "    X = torch.full((B, T), stoi[' '], dtype=torch.long)\n", "    Y = torch.full((B, T), stoi[' '], dtype=torch.long)\n", "    for i,(x,y) in enumerate(zip(xs, ys)):\n", "        X[i, :x.size(0)] = x\n", "        Y[i, :y.size(0)] = y\n", "    return X.to(device), Y.to(device)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Build model"]}, {"cell_type": "code", "metadata": {"id": "build"}, "execution_count": null, "outputs": [], "source": ["model = TinyDialecticalLM(vocab_size, d_model=128, d_head=64, max_steps=4, halt_eps=2e-3, opp_lambda=1e-3).to(device)\n", "sum(p.numel() for p in model.parameters())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Train briefly"]}, {"cell_type": "code", "metadata": {"id": "train"}, "execution_count": null, "outputs": [], "source": ["optim = torch.optim.AdamW(model.parameters(), lr=3e-3)\n", "losses = []\n", "avg_steps_hist = []\n", "for step in range(320):\n", "    X, Y = batchify(B=64, max_len=96)\n", "    logits, metrics, extra = model(X)\n", "    ce = F.cross_entropy(logits.reshape(-1, logits.size(-1)), Y.reshape(-1))\n", "    loss = ce + extra\n", "    optim.zero_grad(); loss.backward(); optim.step()\n", "    losses.append(ce.item())\n", "    avg_steps_hist.append(metrics['avg_steps'])\n", "    if step % 20 == 0:\n", "        print(f\"step {step:03d} | loss {ce.item():.3f} | avg_steps {metrics['avg_steps']:.2f} | tension0 {metrics['tensions'][0]:.3f}\")\n", "\n", "plt.plot(losses); plt.title('Training loss'); plt.show()\n", "plt.plot(avg_steps_hist); plt.title('Avg steps used'); plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Inspect tension profile"]}, {"cell_type": "code", "metadata": {"id": "inspect"}, "execution_count": null, "outputs": [], "source": ["X, Y = batchify(B=32, max_len=96)\n", "with torch.no_grad():\n", "    logits, metrics, _ = model(X)\n", "print('Avg steps (approx):', metrics['avg_steps'])\n", "plt.plot(metrics['tensions']); plt.title('Mean tension per recursion step'); plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Qualitative generation\n", "Greedy next-token generation from a seed."]}, {"cell_type": "code", "metadata": {"id": "sample"}, "execution_count": null, "outputs": [], "source": ["stoi_inv = stoi; itos_inv = {i:ch for ch,i in stoi_inv.items()}\n", "\n", "def generate(model, seed='((', max_new=40):\n", "    model.eval()\n", "    x = torch.tensor([[stoi_inv.get(ch, stoi_inv[' ']) for ch in seed]], device=device)\n", "    with torch.no_grad():\n", "        for _ in range(max_new):\n", "            logits, _, _ = model(x)\n", "            nxt = logits[0, -1].argmax().unsqueeze(0).unsqueeze(0)\n", "            x = torch.cat([x, nxt], dim=1)\n", "    return ''.join(itos_inv[int(i)] for i in x[0].tolist())\n", "\n", "print(generate(model, '(()'))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---\n", "**Tips:**\n", "- Tweak `opp_lambda` (e.g., `5e-4` to `2e-3`) and `halt_eps` to adjust separation and compute.\n", "- Increase `max_steps` to 5\u20136 to see more pronounced convergence behavior (cost rises for hard tokens).\n", "- To compare, replace `TinyDialecticalBlock` with a vanilla attention block and note loss vs. interpretability (no tension/steps)."]}]}