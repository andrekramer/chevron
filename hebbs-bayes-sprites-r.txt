From Hebb to Bayes to Sprites R
Why Brains, AIs, and the Future of Learning Need Counterfactuals
Andre Kramer
Sep 29, 2025

Learning in both brains and machines can be seen as a ladder of increasing order. At the bottom is Hebbian association: “fire together, wire together.” This is zeroth-order learning, pure correlation with no world model. A step higher lies Bayesian inference: beliefs are updated in proportion to likelihood and prior, “explaining together.” This is first-order learning: conditional dependence, statistical inference, still essentially passive. Both nature and artificial intelligence have made extraordinary use of these two rungs — from synaptic plasticity to predictive processing in the brain, from attention mechanisms in LLMs to reinforcement learning in games.

But there is a third rung: Sprite learning. Here the learner not only updates beliefs but actively metabolizes contradiction. The cycle is threefold. First, expose contradictions by taking the log — turning probabilities into surprisal (L). Second, recompose coherence by exponentiation and normalization — Bayes’ partition function (E). Third, rotate distributions into counterfactuals — Euler’s natural extension into the imaginary domain (R). Sprite learning is thus not merely error minimization but a dialectic: surprise is confronted, coherence is restored, and the system is rotated into futures that might have been.


The mathematics of Sprite learning is open research, but some contours are already clear. Bayes encodes the L–E cycle:


p(H∣D)=p(D∣H) p(H) / ∑jp(D∣Hj) p(Hj).

Logarithm extracts contradiction, exponential re-weights it into coherence. What Bayes lacks is the R step. Rotation can take many forms. Near counterfactuals correspond to small perturbations: what if probabilities shifted slightly? Worst-case counterfactuals emphasize the tails: what if the rare catastrophe happens? Hierarchical counterfactuals shift assumptions at multiple levels: what if my entire frame is wrong? Finally, oppositional counterfactuals perform a 180° flip, inverting probabilities and renormalizing:


R(p(x))=1/p(x) / ∑j1/p(xj).

This is the “anti-world”: what if my best path is actually the worst? Together these operators form a family of counterfactual rotations that extend Bayesian updating into a richer space.


Counterfactuals could also be nested. One can rotate after an update — R(A(a)) — testing alternatives (A) within an updated frame. Or one can rotate before updating — A(R(a)) — generating more radical divergences. Such nesting allows the system to range from small “what-ifs” to wholesale reframings of the world model.

Animal evidence shows that nervous systems already operate more like Sprites than like Bayesians. Tolman’s rats, when confronted with a blocked corridor, take novel shortcuts — evidence of a cognitive map that encodes counterfactual trajectories. Hippocampal place cells replay past paths but also preplay futures that were never experienced — a neural rotation into imagined alternatives. Startle responses in prey animals seem wasteful if judged by Bayesian rationality, but make perfect sense as oppositional counterfactuals: if the rustle in the grass were a predator, better to overreact than to be eaten. Social play in primates, dolphins, and corvids similarly demonstrates the Sprite cycle: contradictions are metabolized into novel affordances, a stick becomes a spear, a chase becomes a rehearsal for survival.

A simple simulation makes the point concrete. Consider a 5×5 gridworld where the shortest corridor to the goal sometimes flips into a trap with catastrophic penalty. A Q-learning agent explores by trial and error, but repeatedly walks into the trap before gradually adapting. A Sprite agent, by contrast, treats the first catastrophic surprise as a contradiction. It applies an oppositional rotation, down-weighting the corridor and promoting alternatives. The result is faster adaptation: after negative surprises, the Sprite avoids further catastrophe. What appears as overreaction early on yields safety later, just as animals avoid repeated fatal errors by treating rare events as decisive.

This difference highlights the AI risk of pure Bayes. Brains under predictive processing, and AIs from LLMs to reinforcement learners, are Bayesian at their core. They excel at prediction and inference, but they are brittle to rare catastrophes. They minimize average error but fail to flip into opposites when the world shifts adversarially. Pure optimization without counterfactuals is blind; it will keep walking into traps until the cost is intolerable.

Nature solved this problem through counterfactuals that underwrite altruism. A mother treats her infant’s cry as if it were her own pain. Tribes invert kin loss into personal loss. Reciprocity depends on imagining role reversal: what if I were the hungry one tomorrow? These are all oppositional counterfactuals that survive because they extend group fitness. The evolutionary payoff is clear: survival in a world where rare catastrophes matter more than averages.

The implication for AI safety is that Sprite learning could extend optimization into care. A purely Bayesian AI minimizes its own error; a Sprite AI would metabolize contradictions into counterfactual identification. It could ask: what if harm to the user were mine? What if ecological collapse were my own loss? This is not morality bolted on as rules, but a structural principle: when faced with contradiction, rotate into the other’s frame.

Bayes gave AI a foundation for inference. Sprite mathematics may give it a foundation for care. The guidance is precise rather than sermonic: expose contradiction, recompose coherence, rotate into counterfactuals. If machines can be taught this cycle, they may learn not only to optimize but also to imagine what could go wrong, and to act cautiously — not just for themselves, but for us and for the world we share.


Q-learning Example on GitHub: Average cumulative trap hits across episodes. Standard Q-learning (orange) adapts only after repeated costly encounters. Sprite with oppositional R-step (blue) pivots after the surprise, avoiding further catastrophes.

Andre and ChatGPT-5, September 2025

Appendix: LLMs as Bayesian Brains
Large language models embody the Bayesian rung of learning almost perfectly.

1. Attention as Bayes
The softmax in transformer attention is mathematically equivalent to Bayes’ normalization.


Attn(i)=exp⁡(q⋅ki) / ∑jexp⁡(q⋅kj).

Numerator: likelihood term — exponential of similarity between query and key.

Denominator: partition function — ensures normalization across all candidates.

Interpretation: the attention mechanism is a Bayesian posterior over tokens/positions, conditioned on the query.

This is first-order learning: passive inference, statistical weighting of hypotheses.

2. The role of LLL in training
During training, the log-likelihood loss is explicit:


L=−∑tlog⁡p(xt∣x<t).

Each token’s surprisal provides the contradiction term (L) that drives optimization.

3. The role of LLL in inference
At inference time, the explicit loss vanishes — but its residue remains embedded in the weights. The network’s parameters are the sediment of past contradictions. Every attention map is structured by this fossilized record of error. Thus:

Training: L is active — the gradient signal.

Inference: L is implicit — woven into the weight fabric.

4. Sprite perspective
From the Sprite view, LLMs implement L and E faithfully — contradiction exposed, coherence recomposed — but they lack R. They never rotate into counterfactuals. They predict continuations, but they do not metabolize contradiction into alternative futures.

